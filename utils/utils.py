# import os
# from torchvision.datasets.utils import download_url
# import torch
# import math
# import warnings


# # code from SiT repository
# pretrained_models = {'last.pt'}

# def download_model(model_name):
#     """
#     Downloads a pre-trained SiT model from the web.
#     """
#     assert model_name in pretrained_models
#     local_path = f'pretrained_models/{model_name}'
#     if not os.path.isfile(local_path):
#         os.makedirs('pretrained_models', exist_ok=True)
#         web_path = f'https://www.dl.dropboxusercontent.com/scl/fi/cxedbs4da5ugjq5wg3zrg/last.pt?rlkey=8otgrdkno0nd89po3dpwngwcc&st=apcc645o&dl=0'
#         download_url(web_path, 'pretrained_models', filename=model_name)
#     model = torch.load(local_path, map_location=lambda storage, loc: storage)
#     return model


# @torch.no_grad()
# def load_encoders(enc_type, device):
#     enc_names = enc_type.split(',')
#     encoders, architectures, encoder_types = [], [], []
#     for enc_name in enc_names:
#         encoder_type, architecture, model_config = enc_name.split('-')
#         architectures.append(architecture)
#         encoder_types.append(encoder_type)

#         if 'dinov2' in encoder_type:
#             import timm
#             if 'reg' in encoder_type:
#                 encoder = torch.hub.load('facebookresearch/dinov2', f'dinov2_vit{model_config}14_reg')
#             else:
#                 encoder = torch.hub.load('facebookresearch/dinov2', f'dinov2_vit{model_config}14')
#             del encoder.head
#             encoder.pos_embed.data = timm.layers.pos_embed.resample_abs_pos_embed(
#                 encoder.pos_embed.data, [16, 16],
#             )
#             encoder.head = torch.nn.Identity()
#             encoder = encoder.to(device)
#             encoder.eval()

#         encoders.append(encoder)
    
#     return encoders, encoder_types, architectures


# def _no_grad_trunc_normal_(tensor, mean, std, a, b):
#     # Cut & paste from PyTorch official master until it's in a few official releases - RW
#     # Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf
#     def norm_cdf(x):
#         # Computes standard normal cumulative distribution function
#         return (1. + math.erf(x / math.sqrt(2.))) / 2.

#     if (mean < a - 2 * std) or (mean > b + 2 * std):
#         warnings.warn("mean is more than 2 std from [a, b] in nn.init.trunc_normal_. "
#                       "The distribution of values may be incorrect.",
#                       stacklevel=2)

#     with torch.no_grad():
#         # Values are generated by using a truncated uniform distribution and
#         # then using the inverse CDF for the normal distribution.
#         # Get upper and lower cdf values
#         l = norm_cdf((a - mean) / std)
#         u = norm_cdf((b - mean) / std)

#         # Uniformly fill tensor with values from [l, u], then translate to
#         # [2l-1, 2u-1].
#         tensor.uniform_(2 * l - 1, 2 * u - 1)

#         # Use inverse cdf transform for normal distribution to get truncated
#         # standard normal
#         tensor.erfinv_()

#         # Transform to proper mean, std
#         tensor.mul_(std * math.sqrt(2.))
#         tensor.add_(mean)

#         # Clamp to ensure it's in the proper range
#         tensor.clamp_(min=a, max=b)
#         return tensor


# def trunc_normal_(tensor, mean=0., std=1., a=-2., b=2.):
#     return _no_grad_trunc_normal_(tensor, mean, std, a, b)

